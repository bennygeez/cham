{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--ogP6E-c_zH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import cv2\n",
        "%matplotlib inline\n",
        "\n",
        "import math\n",
        "\n",
        "def grayscale(img):\n",
        "    \"\"\"Applies the Grayscale transform\n",
        "    This will return an image with only one color channel\n",
        "    but NOTE: to see the returned image as grayscale\n",
        "    (assuming your grayscaled image is called 'gray')\n",
        "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
        "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
        "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "def canny(img, low_threshold, high_threshold):\n",
        "    \"\"\"Applies the Canny transform\"\"\"\n",
        "    return cv2.Canny(img, low_threshold, high_threshold)\n",
        "\n",
        "def gaussian_blur(img, kernel_size):\n",
        "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
        "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
        "\n",
        "def region_of_interest(img, vertices):\n",
        "    \"\"\"\n",
        "    Applies an image mask.\n",
        "\n",
        "    Only keeps the region of the image defined by the polygon\n",
        "    formed from `vertices`. The rest of the image is set to black.\n",
        "    `vertices` should be a numpy array of integer points.\n",
        "    \"\"\"\n",
        "    #defining a blank mask to start with\n",
        "    mask = np.zeros_like(img)\n",
        "\n",
        "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
        "    if len(img.shape) > 2:\n",
        "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
        "        ignore_mask_color = (255,) * channel_count\n",
        "    else:\n",
        "        ignore_mask_color = 255\n",
        "\n",
        "    #filling pixels inside the polygon defined by \"vertices\" with the fill color\n",
        "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
        "\n",
        "    #returning the image only where mask pixels are nonzero\n",
        "    masked_image = cv2.bitwise_and(img, mask)\n",
        "    return masked_image\n",
        "\n",
        "\n",
        "def draw_lines(img, lines, color=[255, 0, 0], thickness=5):\n",
        "    \"\"\"\n",
        "    NOTE: this is the function you might want to use as a starting point once you want to\n",
        "    average/extrapolate the line segments you detect to map out the full\n",
        "    extent of the lane (going from the result shown in raw-lines-example.mp4\n",
        "    to that shown in P1_example.mp4).\n",
        "\n",
        "    Think about things like separating line segments by their\n",
        "    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left\n",
        "    line vs. the right line.  Then, you can average the position of each of\n",
        "    the lines and extrapolate to the top and bottom of the lane.\n",
        "\n",
        "    This function draws `lines` with `color` and `thickness`.\n",
        "    Lines are drawn on the image inplace (mutates the image).\n",
        "    If you want to make the lines semi-transparent, think about combining\n",
        "    this function with the weighted_img() function below\n",
        "    \"\"\"\n",
        "    frame_lines      = np.copy(img)\n",
        "    frame_lines[:,:] = [0, 0, 0]\n",
        "\n",
        "    frame_lines_1st = np.copy(img)\n",
        "    frame_lines_1st[:,:] = [0, 0, 0]\n",
        "\n",
        "    frame_lines_2nd = np.copy(img)\n",
        "    frame_lines_2nd[:,:] = [0, 0, 0]\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            for x1,y1,x2,y2 in line:\n",
        "                frame_lines[y1, x1] = [255, 255, 255]\n",
        "                frame_lines[y2, x2] = [255, 255, 255]\n",
        "                thickness_i = 2\n",
        "                cv2.line(frame_lines_1st, (x1, y1), (x2, y2), [255, 255, 255], thickness_i)\n",
        "\n",
        "    frame_lines_1st     = grayscale(frame_lines_1st)\n",
        "    # Gaussian filter\n",
        "    kernel_size     = 1\n",
        "    #frame_lines_1st = gaussian_blur(frame_lines_1st, kernel_size)\n",
        "\n",
        "    # Canny edge detection\n",
        "    low_threshold   = 25\n",
        "    high_threshold  = 150\n",
        "    #frame_lines_1st = canny(frame_lines_1st, low_threshold, high_threshold)\n",
        "\n",
        "    # Hough Transform\n",
        "    minLineLength   = 1\n",
        "    maxLineGap      = 150\n",
        "    rho             = 1\n",
        "    theta           = np.pi/1440\n",
        "    minimum_vote    = 100\n",
        "    lines_2nd_hough = hough_lines(frame_lines_1st, rho, theta, minimum_vote, minLineLength, maxLineGap)\n",
        "\n",
        "    if lines_2nd_hough is not None:\n",
        "        for line in lines_2nd_hough:\n",
        "            for x1,y1,x2,y2 in line:\n",
        "                cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
        "    \"\"\"\n",
        "    `img` should be the output of a Canny transform.\n",
        "\n",
        "    Returns an image with hough lines drawn.\n",
        "    \"\"\"\n",
        "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
        "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
        "    #draw_lines(line_img, lines)\n",
        "    return lines #line_img\n",
        "\n",
        "# Python 3 has support for cool math symbols.\n",
        "\n",
        "def weighted_img(img, initial_img, α=0.8, β=1., γ=0.):\n",
        "    \"\"\"\n",
        "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
        "    Should be a blank image (all black) with lines drawn on it.\n",
        "\n",
        "    `initial_img` should be the image before any processing.\n",
        "\n",
        "    The result image is computed as follows:\n",
        "\n",
        "    initial_img * α + img * β + γ\n",
        "    NOTE: initial_img and img must be the same shape!\n",
        "    \"\"\"\n",
        "    return cv2.addWeighted(initial_img, α, img, β, γ)\n",
        "\n",
        "def color_selection(frame, RGB_thd):\n",
        "# Define color selection threshold\n",
        "# Example : to keep white and yellow RGB_thd should be\n",
        "# RGB_thd = [[200, 200, 200], [200, 200, 0]]\n",
        "\n",
        "    color_selection_ind = np.zeros((frame.shape[0], frame.shape[1]), dtype=bool)\n",
        "    result              = np.copy(frame)\n",
        "\n",
        "    # Define selection by color / below the threshold\n",
        "    for RGB_thd_i in RGB_thd:\n",
        "        color_selection_ind[:,:] = ((result[:,:,0] > RGB_thd_i[0]) & \\\n",
        "                                    (result[:,:,1] > RGB_thd_i[1]) & \\\n",
        "                                    (result[:,:,2] > RGB_thd_i[2])) \\\n",
        "                                    | color_selection_ind[:,:]\n",
        "    result[~color_selection_ind] = [0, 0, 0]\n",
        "    #result                     = np.copy(result[color_selection_ind])\n",
        "    return result, color_selection_ind\n",
        "\n",
        "def process_image(image):\n",
        "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
        "    # TODO: put your pipeline here,\n",
        "    # you should return the final output (image where lines are drawn on lanes)\n",
        "    result = image\n",
        "\n",
        "    # Colour Selection\n",
        "    RGB_thd               = [[125, 125, 125], [150, 150, 0]]\n",
        "    #RGB_thd               = [[115, 115, 115]]\n",
        "    result, color_selection_ind = color_selection(result, RGB_thd)\n",
        "\n",
        "    # Gray scale\n",
        "    result = grayscale(result)\n",
        "\n",
        "    # Gaussian filter\n",
        "    kernel_size     = 11\n",
        "    result = gaussian_blur(result, kernel_size)\n",
        "\n",
        "    # Canny edge detection\n",
        "    low_threshold   = 5\n",
        "    high_threshold  = 15\n",
        "    result = canny(result, low_threshold, high_threshold)\n",
        "\n",
        "    # Zone of interest filtering\n",
        "    vertices        = np.array([[(0,np.int32(image.shape[0]/2) + 100), \\\n",
        "                      (0,image.shape[0]-1), \\\n",
        "                      (image.shape[1]-1,image.shape[0]-1), \\\n",
        "                      (image.shape[1]-1 - 0,np.int32(image.shape[0]/2) + 100), \\\n",
        "                      (np.int32(image.shape[1]/2) + 100, np.int32(image.shape[0]/2) + 50), \\\n",
        "                      (np.int32(image.shape[1]/2) - 100, np.int32(image.shape[0]/2) + 50)]], \\\n",
        "                    dtype=np.int32)\n",
        "    result = region_of_interest(result, vertices)\n",
        "\n",
        "    # Hough Transform\n",
        "    minLineLength   = 1\n",
        "    maxLineGap      = 150\n",
        "    rho             = 1\n",
        "    theta           = np.pi/1440\n",
        "    minimum_vote    = 50\n",
        "    lines           = hough_lines(result, rho, theta, minimum_vote, minLineLength, maxLineGap)\n",
        "\n",
        "    # Draw lines\n",
        "    line_img = np.zeros((image.shape[0], image.shape[1], 3), dtype=np.uint8)\n",
        "    draw_lines(line_img, lines)\n",
        "\n",
        "    # Display the result on original video\n",
        "    result          = weighted_img(line_img, image)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moviepy==1.0.3\n",
        "!pip install imageio-ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l_yaudGdkcL",
        "outputId": "219b9845-f96b-4bd2-8ef6-b4bf330fe1ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy==1.0.3 in ./miniconda3/lib/python3.12/site-packages (1.0.3)\r\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in ./miniconda3/lib/python3.12/site-packages (from moviepy==1.0.3) (4.4.2)\r\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in ./miniconda3/lib/python3.12/site-packages (from moviepy==1.0.3) (4.66.5)\r\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in ./miniconda3/lib/python3.12/site-packages (from moviepy==1.0.3) (2.32.3)\r\n",
            "Requirement already satisfied: proglog<=1.0.0 in ./miniconda3/lib/python3.12/site-packages (from moviepy==1.0.3) (0.1.10)\r\n",
            "Requirement already satisfied: numpy>=1.17.3 in ./miniconda3/lib/python3.12/site-packages (from moviepy==1.0.3) (1.26.3)\r\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in ./miniconda3/lib/python3.12/site-packages (from moviepy==1.0.3) (2.34.1)\r\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in ./miniconda3/lib/python3.12/site-packages (from moviepy==1.0.3) (0.5.1)\r\n",
            "Requirement already satisfied: pillow>=8.3.2 in ./miniconda3/lib/python3.12/site-packages (from imageio<3.0,>=2.5->moviepy==1.0.3) (10.4.0)\r\n",
            "Requirement already satisfied: setuptools in ./miniconda3/lib/python3.12/site-packages (from imageio-ffmpeg>=0.2.0->moviepy==1.0.3) (69.5.1)\r\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.12/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2.0.4)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.12/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.7)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.12/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2.1.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.12/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2024.6.2)\n",
            "Requirement already satisfied: imageio-ffmpeg in ./miniconda3/lib/python3.12/site-packages (0.5.1)\n",
            "Requirement already satisfied: setuptools in ./miniconda3/lib/python3.12/site-packages (from imageio-ffmpeg) (69.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "from IPython.display import HTML\n",
        "#from moviepy import *"
      ],
      "metadata": {
        "id": "tPALTs5CddKG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Preprocessed-video Credit : Udacity')\n",
        "#Load video\n",
        "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
        "clip1 = VideoFileClip(\"/home/avidmech/rr4 (1).mp4\")\n",
        "print('Preprocessed-video Credit : Udacity')\n",
        "\n",
        "#Process loaded video\n",
        "white_clip   = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
        "\n",
        "#Write output video\n",
        "white_output = 'solidWhiteRight_out.mp4'\n",
        "%time white_clip.write_videofile(white_output, audio=False)\n",
        "\n",
        "#Display processed video\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"640\" controls>\n",
        "  <source src=\"{0}\">\n",
        "</video>\n",
        "\"\"\".format(white_output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "thapf9wfeOF9",
        "outputId": "39f345fa-c049-4615-a46f-f2599a6e4b91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed-video Credit : Udacity\n",
            "Preprocessed-video Credit : Udacity\n",
            "Moviepy - Building video solidWhiteRight_out.mp4.\n",
            "Moviepy - Writing video solidWhiteRight_out.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready solidWhiteRight_out.mp4\n",
            "CPU times: user 39min 26s, sys: 1min 36s, total: 41min 2s\n",
            "Wall time: 36min 54s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"640\" controls>\n",
              "  <source src=\"solidWhiteRight_out.mp4\">\n",
              "</video>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def color_keep_range(frame, RGB_thd):\n",
        "# Define color selection threshold - keep in-range pixels\n",
        "# Example : to keep white and yellow RGB_thd should be\n",
        "# RGB_thd = [([200, 200, 200], [255, 255, 255]), ([150, 150, 100], [180, 180, 120])]\n",
        "\n",
        "    color_selection_ind = np.zeros((frame.shape[0], frame.shape[1]), dtype=bool)\n",
        "    result              = np.copy(frame)\n",
        "\n",
        "    # Define selection by color / below the threshold\n",
        "    for RGB_thd_i in RGB_thd:\n",
        "        color_selection_ind[:,:] = ((result[:,:,0] > RGB_thd_i[0][0]) & (result[:,:,0] < RGB_thd_i[1][0]) & \\\n",
        "                                    (result[:,:,1] > RGB_thd_i[0][1]) & (result[:,:,1] < RGB_thd_i[1][1]) & \\\n",
        "                                    (result[:,:,2] > RGB_thd_i[0][2]) & (result[:,:,2] < RGB_thd_i[1][2]))\\\n",
        "                                    | color_selection_ind[:,:]\n",
        "\n",
        "    result[~color_selection_ind] = [0, 0, 0]\n",
        "    return result, color_selection_ind\n",
        "\n",
        "def color_remove_range(frame, RGB_thd):\n",
        "# Define color selection threshold - remove in-range pixels\n",
        "# Example : to remove white and yellow RGB_thd should be\n",
        "# RGB_thd = [([200, 200, 200], [255, 255, 255]), ([150, 150, 100], [180, 180, 120])]\n",
        "\n",
        "    color_remove_ind         = np.zeros((frame.shape[0], frame.shape[1]), dtype=bool)\n",
        "    color_remove_ind_temp    = np.zeros((frame.shape[0], frame.shape[1]), dtype=bool)\n",
        "    result                   = np.copy(frame)\n",
        "\n",
        "    # Define selection by color / below the threshold\n",
        "    for RGB_thd_i in RGB_thd:\n",
        "        color_remove_ind_temp[:,:] = ((result[:,:,0] > RGB_thd_i[0][0]) & (result[:,:,0] < RGB_thd_i[1][0]) & \\\n",
        "                                    (result[:,:,1] > RGB_thd_i[0][1]) & (result[:,:,1] < RGB_thd_i[1][1]) & \\\n",
        "                                    (result[:,:,2] > RGB_thd_i[0][2]) & (result[:,:,2] < RGB_thd_i[1][2]))\n",
        "        color_remove_ind           = color_remove_ind | color_remove_ind_temp\n",
        "\n",
        "    result[color_remove_ind] = [0, 0, 0]\n",
        "    return result, color_remove_ind\n",
        "\n",
        "def process_image(image):\n",
        "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
        "    # TODO: put your pipeline here,\n",
        "    # you should return the final output (image where lines are drawn on lanes)\n",
        "    result = image\n",
        "\n",
        "    # Colour Selection\n",
        "\n",
        "    # keep white and yellow pixels\n",
        "    RGB_thd_keep = [([150, 150, 150], [255, 255, 255]), ([160, 160, 100], [255, 210, 140])]\n",
        "    result, k                          = color_keep_range(result, RGB_thd_keep)\n",
        "\n",
        "    # remove unwanted colours\n",
        "    RGB_thd_remove = [([0, 0, 0], [215, 215, 180])]\n",
        "    result, k                          = color_remove_range(result, RGB_thd_remove)\n",
        "\n",
        "    # Gray scale\n",
        "    result = grayscale(result)\n",
        "\n",
        "    # Gaussian filter\n",
        "    kernel_size     = 13\n",
        "    result = gaussian_blur(result, kernel_size)\n",
        "\n",
        "    # Canny edge detection\n",
        "    low_threshold   = 20\n",
        "    high_threshold  = 30\n",
        "    result = canny(result, low_threshold, high_threshold)\n",
        "\n",
        "    # Zone of interest filtering\n",
        "    vertices        = np.array([[(0,np.int32(image.shape[0]/2) + 100), \\\n",
        "                      (0,image.shape[0]-1), \\\n",
        "                      (image.shape[1]-1,image.shape[0]-1), \\\n",
        "                      (image.shape[1]-1 - 0,np.int32(image.shape[0]/2) + 100), \\\n",
        "                      (np.int32(image.shape[1]/2) + 100, np.int32(image.shape[0]/2) + 50), \\\n",
        "                      (np.int32(image.shape[1]/2) - 100, np.int32(image.shape[0]/2) + 50)]], \\\n",
        "                    dtype=np.int32)\n",
        "    result = region_of_interest(result, vertices)\n",
        "\n",
        "    # Hough Transform\n",
        "    minLineLength   = 1\n",
        "    maxLineGap      = 15\n",
        "    rho             = 1\n",
        "    theta           = np.pi/1440\n",
        "    minimum_vote    = 30\n",
        "    lines           = hough_lines(result, rho, theta, minimum_vote, minLineLength, maxLineGap)\n",
        "\n",
        "    # Draw lines\n",
        "    line_img = np.zeros((image.shape[0], image.shape[1], 3), dtype=np.uint8)\n",
        "    draw_lines(line_img, lines)\n",
        "\n",
        "    # Display the result on original video\n",
        "    result          = weighted_img(line_img, image)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "3Z4Fyeqvolpf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Preprocessed-video Credit : Udacity')\n",
        "#Load video\n",
        "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
        "clip1_new        = VideoFileClip(\"/home/avidmech/left4.mp4\")\n",
        "  ` ``````````````````\n",
        "#Process loaded video\n",
        "white_clip_new   = clip1_new.fl_image(process_image) #NOTE: this function expects color images!!\n",
        "\n",
        "#Write output video\n",
        "white_output_new = 'solidWhiteLeft_out_new.mp4'\n",
        "%time white_clip_new.write_videofile(white_output_new, audio=False)\n",
        "\n",
        "#Display processed video\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"640\" controls>\n",
        "  <source src=\"{0}\">\n",
        "</video>\n",
        "\"\"\".format(white_output_new))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "UcbCwypMosos",
        "outputId": "2c3dbe10-d49e-4dc4-d3e9-1053725469cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed-video Credit : Udacity\n",
            "Moviepy - Building video solidWhiteLeft_out_new.mp4.\n",
            "Moviepy - Writing video solidWhiteLeft_out_new.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready solidWhiteLeft_out_new.mp4\n",
            "CPU times: user 2h 44min 17s, sys: 25min 2s, total: 3h 9min 19s\n",
            "Wall time: 2h 36min 21s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"640\" controls>\n",
              "  <source src=\"solidWhiteLeft_out_new.mp4\">\n",
              "</video>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHfWMqOqrott"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}